\documentclass[a4paper,12pt]{article}

\usepackage{amsmath,amsthm,amsfonts,dsfont,ae,aecompl,url,hyperref}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{tikz,sgame}

\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{ex}{Example}
\theoremstyle{plain}


\title{Math for Microeconomics}
\author{Christoph Schottm\"uller}

\begin{document}
\maketitle

\section{Uniform distribution}
\label{sec:uniform-distribution}

The uniform distribution is in some sense the simplest distribution a continuous random variable can have. Recall that a cumulative distribution function (cdf) $F$ of a random variable $X$ gives the answer to the question ``What is the probability that  $X$ is weakly less than some value $x$''. Put differently, $F(x)=0.3$ states that the probability that $X$ is weakly less than $x$ equals $0.3$. Given this interpretation, we immediately know a few properties that a cumulative distribution function has to satisfy:
\begin{enumerate}
\item $F(x)\in[0,1]$: as $F(x)$ is a probability, it has to be between 0 and 1.
\item $F$ is increasing: if the probability that $X\leq x$ is $0.3$, then the probability that $X\leq y$ for $y>x$ has to be at least $0.3$. (Just think about the statement ``The probability that it rains before 3pm is 50\%.'' Logically, this implies that the probability that it rains before 5pm has to be at least 50\%.)
\item If $X$ takes only values in the interval $[a,b]$ (we then call the smallest such interval the \emph{support} of $X$), then $F(x)=0$ for all $x<a$ and $F(x)=1$ for all $x>b$. (If we know for sure that it is raining at some point between 1pm and 3pm, then the probability that it starts raining before 5pm is 1.) 
  \item Let $x>y$. Then $F(x)-F(y)$ is the probability that $X$ is in the interval $(y,x]$. (The probability that it rains before 5pm minus the probability that it rains before 3pm equals the probability that it rains between 3pm and 5pm.)
\end{enumerate}

If the cdf $F$ of the random variable $X$ is continuous and differentiable (almost everywhere), then the derivative of $F$ is called the \emph{density} function of $X$ and typically we denote this density function by $f$. If the support of $X$ is $[a,b]$ and $X$ has the density $f$, then we can write $F(x)=\int_{a}^xf(\tilde x)\,d\tilde{x}$.

The uniform distribution is a particularly simple distribution because its cumulative distribution function is (i) continuous and (ii) linear on its support. In fact, it has a density and the density is constant on the support as the derivative of a linear function is constant. The simplest example of a uniform distribution is the uniform distribution on $[0,1]$ (put differently, we have a random variable $X$ with support $[0,1]$ and and a continuous cdf that is linear on the $[0,1]$). For this uniform distribution on $[0,1]$, we have
$$F(x)=
\begin{cases}
  0 & x<0\\
  x & x\in[0,1]\\
  1 &x>0
\end{cases}$$
and therefore
$$f(x)=
\begin{cases}
  1 & x\in[0,1]\\
  0 & else.
\end{cases}$$

It is tempting to think of a uniform distribution as a distribution for which all values in the support are equally likely. Technically speaking, this intuition is problematic because each value has itself zero probability, i.e. the probability that $X$ equals 0.4546 is zero. Therefore, a better intuition is the following: Fix some interval length, for example, 0.1. The probability that $X$ falls into any subinterval of its support with length 0.1 is the same. That is, the probability that $X$ is in $[0.2,0.3]$ equals the probability that $X$ is in the interval $[0.56,0.66]$ (if both of these intervals are in the support of $X$) because both have length 0.1. If the support of a uniformly distributed random variable has length 1, e.g. in case of the uniform distribution on $[0,1]$, then the length of the subinterval is in fact equal to the probability that $X$ falls into it. For example, the probability that $X$ is in $[0.45,0.49]$ equals $0.04$ in this case.

Finally, let us write down the cdf and density of a uniform distribution on an arbitrary interval $[a,b]$. We know that (i) $F$ is linear on $[a,b]$ and can therefore be written as $F(x)=\alpha+\beta x$ for $x\in[a,b]$, (ii) $F(x)=0$ for $x<a$ and, as $F$ is continuous, $F(a)=0$, (iii) $F(x)=1$ for $x>1$ and $F(b)=1$ by the continuity of $F$. The equations $F(a)=0$ and $F(b)=1$ can be written as
\begin{align*}
  \alpha+\beta a&=0\\
  \alpha+\beta b&=1.
\end{align*}
Solving these equations for $\alpha$ and $\beta$, we obtain
\begin{align*}
  \alpha&=-\frac{a}{b-a}\\
  \beta&=\frac{1}{b-a}.        
\end{align*}
Hence, $F(x)=(x-a)/(b-a)$ and $f(x)=1/(b-a)$ for $x\in[a,b]$.

\subsection{Expected value}
\label{sec:expected-value}

Recall that the expected value of a random variable $X$ with density $f$ and support $[a,b]$ can be computed as $\mathbb{E}[X]=\int_a^bxf(x)\,dx$. For the case of the uniform distribution this yields
$$\int_a^bx\frac{1}{b-a}\,dx=\frac{1}{b-a}\int_a^bx\,dx=\frac{1}{b-a} [\frac{1}{2}x^2]_a^b=\frac{1}{2}\frac{b^2-a^2}{b-a}=\frac{1}{2}\frac{(b-a)(b+a)}{b-a}=\frac{b+a}{2}.$$

This should not be too surprising: The expected value of a uniform distribution is the midpoint of its support. 

\subsection{Conditional expectation}
\label{sec:cond-expect}

The expected value of $X$ conditional on some condition $C$ is written $\mathbb{E}[X|C]$. This expression answers the question ``What is the expected value of X if we know that X satisfies the condition C?'' For example, one might be interested in the expected value of wage conditional on having a university degree. I will focus here on very simple conditions, namely that $X$ is in some interval $[c,d]$. That is, I am interested in $\mathbb{E}[X|c\leq X\leq d]$. For example, one may be interested in the expected wage of those above the poverty line. If the random variable $X$ has density $f$ and support $[a,b]$, then 
$$\mathbb{E}[X|c\leq X\leq d]=\frac{\int_c^dxf(x)\,dx}{F(d)-F(c)}.$$
In case of a uniform distribution and $[c,d]\subseteq[a,b]$, this expression becomes
$$\frac{\int_c^dxf(x)\,dx}{F(d)-F(c)}=\frac{(d^2/2-c^2/2)/(b-a)}{(d-c)/(b-a)}=\frac{1}{2}\frac{d^2-c^2}{d-c}=\frac{1}{2}\frac{(d+c)(d-c)}{d-c}=\frac{d+c}{2}.$$

Again, this should not surprise too much: The expected value of a uniformly distributed variable under the condition that its value is in $[c,d]$ is the midpoint of this interval. 

\section{Convergence of sequences}
\label{sec:conv-sequ}

We start by defining what we mean when we say a sequence \emph{converges} (if you have no idea what a ``sequence'' is, you might want to check the wikipedia entry \url{https://en.wikipedia.org/wiki/Sequence}; note that in this handout we are interested in ``infinite sequences'' only): A sequence $(a_n)_{n=1}^\infty$, where each $a_n\in \Re$, converges to $a\in \Re$ if for every $\varepsilon >0$ we can find an $N$ such that
\begin{equation}
  \label{eq:1}
  |a_n-a|\leq\varepsilon \quad\text{ for all }\quad n\geq N.
\end{equation}
We then say that ``$a$ is the \emph{limit} of the sequence $(a_n)_{n=1}^\infty$''.

\textbf{Example: }
  The sequence $2,\frac{3}{2},\frac{4}{3},\frac{5}{4},\dots,\frac{n+1}{n},\dots$ converges to 1: For a given $\varepsilon >0$, $\left|\frac{n+1}{n}-1\right|$ is less than $\varepsilon $ for $n$ high enough. More precisely, $\left|\frac{n+1}{n}-1\right|=\left|\frac{1}{n}\right|=\frac{1}{n}<\varepsilon $ for $n>\frac{1}{\varepsilon }$. Hence, by choosing $N=\left[\frac{1}{\varepsilon }\right]+1$ (where $[\,]$ indicates rounding to the next integer) we know that $|a_n-1|\leq\varepsilon \;\text{ for all }\; n\geq N$ and therefore the limit of the sequence is 1. 

However, the elements of a sequence do not have to be real numbers. We can also have sequences of vectors: A sequence $(a_n)_{n=1}^\infty$, where each $a_n\in \Re^m$, converges to a vector $a\in \Re^m$ if for every $\varepsilon >0$ we can find an $N$ such that 
\begin{equation}
  \label{eq:2}
  ||a_n-a||\leq\varepsilon \quad\text{ for all }\quad n\geq N.
\end{equation}
In the last equation ``$||\cdot||$'' denotes a norm, e.g. for a vector $b=(b_1,\dots,b_m)$ the Euclidean norm is 
\begin{equation}
  \label{eq:3}
  ||b||=\sqrt{b_1^2+\dots+b_m^2}.
\end{equation}

\textbf{Example: }
  Let $a_n=\left(\begin{matrix}\frac{n+1}{n}\\ \frac{1}{n}\end{matrix}\right)$. So, the sequence starts $\left(\begin{matrix}\frac{2}{1}\\ \frac{1}{1}\end{matrix}\right),\,\left(\begin{matrix}\frac{3}{2}\\ \frac{1}{2}\end{matrix}\right),\,\left(\begin{matrix}\frac{4}{3}\\ \frac{1}{3}\end{matrix}\right)\dots$. I claim that this sequence converges to $\left(\begin{matrix} 1\\ 0\end{matrix}\right)$. Note that $||a_n-\left(\begin{matrix} 1\\ 0\end{matrix}\right)||=||\left(\begin{matrix} \frac{1}{n}\\ \frac{1}{n}\end{matrix}\right)||=\sqrt{\frac{1}{n^2}+\frac{1}{n^2}}=\frac{\sqrt{2}}{n}$ which is less or equal to $ \varepsilon>0 $ if $n\geq \frac{\sqrt{2}}{\varepsilon }$. Hence, choosing $N=\left[\frac{\sqrt{2}}{\varepsilon }\right]+1$ we have shown that $||a_n-\left(\begin{matrix} 1\\ 0\end{matrix}\right)||\leq\varepsilon \,\text{ for all }\, n\geq N$ and therefore $(a_n)_{n=1}^\infty$ converges indeed to $\left(\begin{matrix} 1\\ 0\end{matrix}\right)$.


You might have realized something in the previous example: The sequence in the first component of $\left(\begin{matrix}\frac{n+1}{n}\\ \frac{1}{n}\end{matrix}\right)_{n=1}^\infty$ is $a_n^1=\left(\frac{n+1}{n}\right)_{n=1}^\infty$ which was the sequence we looked at in example 1. There we showed that this sequence converges to 1. The sequence in the second component of $a_n=\left(\begin{matrix}\frac{n+1}{n}\\ \frac{1}{n}\end{matrix}\right)$ is $a_n^2=\left(\frac{1}{n}\right)_{n=1}^\infty$ and it is not so hard to see that this sequence converges to 0. If you take these two together, it is hardly surprising that $\left(\begin{matrix}\frac{n+1}{n}\\ \frac{1}{n}\end{matrix}\right)_{n=1}^\infty$ converges to   $\left(\begin{matrix} 1\\ 0\end{matrix}\right)$. The question is: Is this accidental or is it \emph{always} the case that a vector sequence converges to the vector consisting of the limits of its component sequences? The following proposition says that this is always the case.

\begin{proposition}
  A sequence $(a_n)_{n=1}^\infty$ in $\Re^m$ converges to a vector
  $a\in\Re^m$ if and only if all component sequences converge.
\end{proposition}
\textbf{Proof. }Let's write $a_n=(a_n^1,a_n^2,\dots,a_n^m)$. We denote the sequence in the $i-th$ component as $(a_n^i)_{n=1}^\infty$. First we show that $(a_n)_{n=1}^\infty$ converges if each component sequence $(a_n^i)_{n=1}^\infty$ for $i\in\{1,2,\dots,m\}$ converges:\\
Denote the limit of the i-th component sequence as $a^i$ and let $a=(a^1,a^2,\dots,a^m)$. Take an $\varepsilon>0 $. \\
Since $(a_n^i)_{n=1}^\infty$ converges, we can find $N^i$ such that $|a_n^i-a^i|\leq\frac{\varepsilon }{\sqrt{m}} $ for all $n\geq N^i$. Now take $N=\max\{N^1,\dots,N^m\}$. Then for all $n\geq N$, we have
\begin{multline*}
  ||a_n-a||=\sqrt{(a^1_n-a^1)^2+(a^2_n-a^2)^2+\dots+(a^m_n-a^m)^2}\\
\leq
  \sqrt{\left(\frac{\varepsilon
  }{\sqrt{m}}\right)^2+\left(\frac{\varepsilon
  }{\sqrt{m}}\right)^2+\dots+\left(\frac{\varepsilon
  }{\sqrt{m}}\right)^2}
  =\sqrt{m\frac{\varepsilon^2 }{m}}=\varepsilon.
  \end{multline*}
Therefore, $(a_n)_{n=1}^\infty$ converges to $a$.\par
Second we have to show that all component sequences $(a_n^i)_{n=1}^\infty$ converge if $(a_n)_{n=1}^\infty$ converges. This follows from
$$||a_n-a||=\sqrt{(a^1_n-a^1)^2+(a^2_n-a^2)^2+\dots+(a^m_n-a^m)^2}\geq \sqrt{(a_n^i-a^i)^2}=|a_n^i-a^i|.$$
Since $(a_n)_{n=1}^\infty$ converges to $a$, $||a_n-a||\leq \varepsilon $ for all $n\geq N$. But then the last expression implies that also $|a_n^i-a^i|\leq\varepsilon $. Consequently, $(a_n^i)_{n=1}^\infty$ converges to $a^i$.
\qed

We can define closed sets in $\Re^m$ in terms of sequences:
\begin{definition}
  A set $S\in \Re^m$ is \textbf{closed} if and only if every converging
  sequence in $S$ has its limit in $S$.
\end{definition}
Put differently, let $(s_n)_{n=1}^\infty$ be an arbitrary sequence in $S$, i.e. $s_n\in S$ for $n=1,2,\dots$, and let $(s_n)_{n=1}^\infty$ converge to $s$. Then $s\in S$ if $S$ closed. If $s\in S$ for every converging sequence that is entirely contained in $S$, then $S$ is closed.

\textbf{Example: }
  Let's look at the simple case where $m=1$, i.e. we have normal real sequences.  The interval $S=(1,2]$ (including 2 but not including 1) is \emph{not} closed: Our sequence from the first example  $2,\frac{3}{2},\frac{4}{3},\frac{5}{4},\dots,\frac{n+1}{n},\dots$ is in $S$, i.e. each $a_n$ is an element of $S$. But the limit of the sequence is 1 which is not in $S$.\par
Now let's look at the set $S'=[1,2]$. This set is closed and we can show this by contradiction: Suppose, $S'$ was not closed. Then there would be a sequence $(a_n)_{n=1}^\infty$ such that (i) $a_n\in S'$ for all $n$, (ii) $(a_n)_{n=1}^\infty$ converges to \emph{a} and (iii) $a\not\in S'$. Hence, \emph{a} would be either strictly greater than 2 or strictly smaller than 1. Let's say \emph{a} was strictly greater than 2 (the other case is similar). Then define $\varepsilon = (a-2)/2$. As $(a_n)_{n=1}^\infty$ converges to \emph{a}, there would have to be some $a_n$ such that $|a_n-a|<\varepsilon $. But this would imply that $a_n>2$ and therefore $a_n\not\in S'$. But $a_n\not\in S'$ contradicts (i). Hence, $S'$ is closed.



One important concept is a \emph{subsequence}. Intuitively, a subsequence is what is left over after you delete some terms of the original sequence. For example, take the sequence $(1/1,1/2,1/3,1/4,1/5,\dots)$. One subsequence of this sequence would be $(1/1,1/2,1/4,1/8,1/16\dots)$. Another subsequence would be $(1/2,1/4,1/10,1/28,\dots)$ (in case you wondered: the rule here is $1/(3^{n-1}+1)$). For a more formal definition of a subsequence, check your math books, e.g. Simon/Blume ch. 12 (p. 256).

One theorem that is remarkably useful is the Bolzano-Weierstrass theorem.\footnote{Recall: A set $S$ in $\Re^m$ is called \emph{compact} if and only if it is closed and bounded. Boundedness means that there is a $K\in\Re^m$ such that $s<K$ and $s>-K$ for all $s\in S$, i.e. the elements of $S$ are not arbitrarily large (in absolute value).}
\begin{theorem}
  Let $S$ be a compact set in $\Re^m$ and let $(a_n)_{n=1}^\infty$ be a sequence that is entirely contained in $S$, i.e. $a_n\in S$ for all $n=1,2\dots$. Then $(a_n)_{n=1}^\infty$ has a convergent subsequence with limit in $S$.
\end{theorem}

Instead of a full-fledged proof we just look at the idea for the simple case where $m=1$, i.e. the sequence is a sequence of real numbers. A compact set $S$ is then a set that is closed and bounded and we will look at the simple case of a closed interval. For simplicity we let $S=[0,1]$. Now we construct a subsequence $(b_n)_{n=1}^\infty$ of $(a_n)_{n=1}^\infty$ such that $(b_n)_{n=1}^\infty$ converges:\\
\begin{enumerate}
\item Let's split $[0,1]$ in two subintervals $[0,1/2]$ and $[1/2,1]$. At least one of the two subintervals will contain an infinite number of elements of $(a_n)_{n=1}^\infty$. If $[0,1/2]$ contains an infinite number of elements of $(a_n)_{n=1}^\infty$, then we choose $b_1$ as an arbitrary $a_{n_1}$ in the subinterval $[0,1/2]$.  If $[0,1/2]$ does not contain an infinite number of $a_n$s, we let $b_1$ be some arbitrary $a_{n_1}$ in $[1/2,1]$.
\item Now we split $[0,1]$ into four subintervals: $[0,1/4]$, $[1/4,1/2]$, $[1/2,3/4]$ and $[3/4,1]$. We  let $b_2$ be an arbitrary $a_{n_2}$ that satisfies 2 conditions: (i) $n_2>n_1$ and (ii) $a_{n_2}$ is in the lowest of the four subintervals that contains an infinite number of $a_n$s.
\item Now we split $[0,1]$ into 16 equally long closed subintervals. We let $b_3$ be some $a_{n_3}$ such that: (i) $n_3>n_2$ and (ii) $a_{n_3}$ is in the lowest of the 16 subintervals that contains an infinite number of $a_n$s.
\item \dots
\end{enumerate}
The sequence $(b_n)_{n=1}^\infty$ constructed in this way converges: By the construction, any two $b_n$ and $b_{n'}$ for $n>N$ and $n'>N$ are no more than $(1/2)^N$ apart. This means that the sequence $(b_n)_{n=1}^\infty$ is a so called ``Cauchy sequence'' and a fundamental property of the real numbers is that every Cauchy sequence converges.\footnote{In principle, this is something I should prove but I guess that this property is not super surprising and I leave it here.} As $[0,1]$ is closed (and all $b_n$ are in $[0,1]$, the limit of this sequence will be in $[0,1]$.

% \par
% For a more detailled exposition, you can check almost any ``Maths for economists'' book (there are a lot of them). One (of many) that is good to read is\par
% Carl P. Simon and Lawrence Blume: \emph{``Mathematics for Economists'',} W.W. Norton \& Company Inc., 1994\\
% where all these results (and many more) can be found in chapter 12 (and some more advanced results in chapter 29).\footnote{In case you are very muched intrigued by these sort of results and want to know more even after reading Simon/Blume, you can have a look at de la Fuente's ``Mathematical Methods and Models in Economics'' or Efe Ok's ``Real Analysis with Economic Applications''. }



\section{Implicit function theorem}
\label{sec:impl-funct-theor}

Often a best response function is defined as the solution to a first order condition.
Consider, for example, a Cournot competition model with two firms which both have constant marginal costs $c>0$. Inverse demand is $P(q_1+q_2)$ where we assume that the function $P$ is two times continuously differentiable with $P'<0$ (demand is decreasing) and $P''\leq 0$ (weak concavity). Firm $i\in\{1,2\}$ maximizes $q_i*(P(q_1+q_2)-c)$ over $q_i$. The first order condition of firm 1 is then
\begin{equation*}
  P'(q_1+q_2)q_1+P(q_1+q_2)-c=0.
\end{equation*}
Firm 1's best response to a quantity $q_2$ is the $q_1$ that solves this equation, i.e. firm 1's best response is ``implicitly defined'' by this equation. (It would be explicit if we could solve the equation for $q_1$ but with a general function $P$ we cannot do so.) We make assumptions that guarantee that the equation has only one solution, e.g. $P'<0$ and $P''\leq 0$, which ensures that the implicitly defined best response $q_1(q_2)$ is a well defined function. The implicit function theorem allows us to calculate the slope of this best response function.

\begin{theorem}[1-dimensional Implicit Function Theorem]
  Let $(f_0,x_0)$ be a point such that $g(f_0,x_0)=0$ where $g:\Re^2\rightarrow\Re$ is a continuously differentiable function and $\partial g(f_0,x_0)/\partial f\neq 0$. Then there exists a function $f:(x_0-\varepsilon ,x_0+\varepsilon )\rightarrow\Re$ for some $\varepsilon >0$ such that $f(x)$ is the unique solution to $g(f,x)=0$ for $x\in(x_0-\varepsilon ,x_0+\varepsilon )$ and
  \begin{equation*}
    f'(x)=-\frac{\partial g(f(x),x)/\partial x}{\partial g(f(x),x)\partial f}.
  \end{equation*}
\end{theorem}

Intuitively, we define $f$ as the solution to the equation $g(f,x)=0$. To get the slope of $f$ we totally differentiate this equation
\begin{equation*}
  \frac{\partial g}{\partial f}df+\frac{\partial g}{\partial x}dx=0
\end{equation*}
and rearrange to get $df/dx$. The restriction of the domain to $(x_0-\varepsilon, x_0+\varepsilon )$ and the assumption $\partial g(f_0,x_0)/\partial f\neq 0$ ensure that the equation $g(f,x)$ has a unique solution (on this restricted domain) and therefore the solution to the equation $g(f,x)=0$ is  actually  a function (and not a correspondence). If we knew that $g(f,x)=0$ has a unique solution $f(x)$ for all $x$, then we could leave out the domain restriction.

In the Cournot example, $q_1$ has the role of $f$, $q_2$ the role of $x$ and $g(q_1,q_2)= P'(q_1+q_2)q_1+P(q_1+q_2)-c$. Because of the assumptions $P'<0$ and $P''\leq 0$, we know that $\partial g/\partial q_1<0$ and therefore $g(q_1,q_2)=0$ can have only one solution $q_1$ for a given $q_2$.\footnote{I ignore corner solutions here, i.e. solutions where the first order condition does not hold as an equality. These are relevant for very high $q_2$ where $q_1=0$ can be a best response but ignored here as this has little to do with the implicit function theorem itself.} The theorem then tells us that

\begin{equation*}
   q_1'(q_2)=-\frac{\partial g(q_1(q_2),q_2)/\partial q_2}{\partial g(q_1(q_2),q_2)\partial q_1}=-\frac{P''(q_1(q_2)+q_2)q_1+P'(q_1(q_2)+q_2)}{P''(q_1(q_2)+q_2)+2P'(q_1(q_2)+q_2)}.
\end{equation*}

This is a useful expression because, by our assumptions $P'<0$ and $P''\leq0$, it tells us that $q_1'(q_2)<0$, i.e. firm 1's best response to an increase in firm 2's quantity is a decrease in quantity. (We call this property ``strategic substitutability''.)

\section{Integration by parts}
\label{sec:integration-parts}

This is a little trick that allows you to solve some seemingly tricky integration problems. We will use it a lot in the course. 

You might remember how to integrate simple polynomial functions, e.g.\footnote{Reminder on notation: $[f(x)]_a^b\equiv f(b)-f(a)$.}
$$\int_{2}^{3}x^{2}+1\;dx=\left[ \frac{x^{3}}{3}+x\right]_{2}^{3}=\frac{27}{3}+3-\frac{8}{3}-2=8-\frac{2}{3}.$$
However, constructing the ``antiderivative'' is not always this easy. Think for example of the following integral:
$$\int_{4}^{5} x e^{x}\;dx$$
Here the following little rule helps us:
\begin{theorem}
$\int_{a}^{b}u(x)v'(x)\;dx=\left[u(x)v(x)\right]_{a}^{b}-\int_{a}^{b}u'(x)v(x)\;dx$
\end{theorem} 

Here $u$ and $v$ are (almost everywhere) differentiable functions of $x$. How does this help us? Let's take the example above and say $v'(x)=e^x$ and $u(x)=x$. This implies that $v(x)=e^x$ because the exponential function is its own derivative and $u'(x)=1$. Then the formula above gives us
\begin{equation*}
  \int_4^5 x e^x\;dx=\left[x e^x\right]_4^5-\int_4^5 1*e^x\;dx.
\end{equation*}
This last integral is something we can solve: The antiderivative of $e^x$ is $e^x$ and therefore the last expression can be rewritten as
\begin{equation*}
  = 5 e^5-4 e^4-\left[e^x\right]_4^5=4 e^4-5 e^5-e^5+e^4=5 e^4-6 e^5.
\end{equation*}

\textbf{Another example: }
\begin{equation*}
  \int_a^b x^2 log(x)\;dx
\end{equation*}
Here we use $u(x)=log(x)$ and $v'(x)=x^2$. Therefore, we get $u'(x)=1/x$ and $v(x)=x^3/3$. Plugging this into our rule gives
\begin{align*}
  \int_a^b &x^2 log(x)\;dx=\left[log(x)x^3/3\right]_a^b- \int_a^b\frac{1}{x}\frac{x^3}{3}\;dx=log(b)b^3/3-log(a)a^3/3-\int_a^b\frac{x^2}{3}\;dx\\
&=log(b)b^3/3-log(a)a^3/3-\left[\frac{x^3}{9}\right]_a^b=log(b)b^3/3-log(a)a^3/3-\frac{b^3}{9}+\frac{a^3}{9}.
\end{align*}

\textbf{Example: Using integration by parts to eliminate a double integral}

Integration by parts can sometimes be used to transform double integrals to simple integrals:
\begin{equation*}
  \int_0^1\left(2x \int_a^x f(y)\;dy\right)\;dx
\end{equation*}
Here $f(y)$ is some measurable function. We choose $u(x)=\int_a^xf(y)\;dy$ and $v'(x)=2x$. This implies that $u'(x)=f(x)$ (this uses the Leibniz rule above!) and $v(x)=x^2$. Integration by parts gives now
\begin{equation*}
  \int_0^1\left(2x \int_a^x f(y)\;dy\right)\;dx= \left[x^2 \int_a^x f(y)\;dy\right]_0^1-\int_0^1f(x) x^2\;dx=\int_a^1f(y)\;dy-\int_0^1f(x)x^2\;dx.
\end{equation*}

If you check the last expression, you see that we simplified the complicated double intergral we started out with into two simpler ``normal'' integrals.


\subsection{Why does integration by parts work? (optional and mathematically somewhat loose)}
\label{sec:why-doe-sintegration}
Integration by parts is actually a rule that you already know (but in disguise). You might remember the differentiation rule:
\begin{equation*}
  \left(u(x) v(x)\right)'=u'(x)v(x)+u(x)v'(x)
\end{equation*}

Now rearrange this last equation by subtracting $u'(x)v(x)$ on both sides. This gives
\begin{equation*}
  \left(u(x) v(x)\right)'-u'(x)v(x)=u(x)v'(x).
\end{equation*}
Now integrate both sides from $a$ to $b$:
\begin{equation*}
  \int_a^b\left\{(u(x) v(x))'-u'(x)v(x)\right\}\;dx=\int_a^bu(x)v'(x)\;dx
\end{equation*}
Now we can split up the integral on the left hand side and get
\begin{equation*}
  \int_a^b\left(u(x) v(x)\right)' \;dx-\int_a^bu'(x)v(x)\;dx=\int_a^bu(x)v'(x)\;dx.
\end{equation*}
Taking derivatives and integrating are inverse operations (``they cancel each other out''). Therefore, the first term on the left hand side can be rewritten $\int_a^b\left(u(x) v(x)\right)' \;dx=\left[ u(x) v(x)\right]_a^b$. If you plug this in, you get exactly the integration by parts formula!



\section{Leibniz rule and Fundamental theorem of calculus}
\label{sec:leibniz-rule}

The Leibniz rule tells you how to take the derivative of a function that is an integral. We will only use one very special case of it. Therefore, I show you this special case and give the complete rule only for sake of completeness below.

What we want to look at are functions like the following
$$f(x)=\int_0^x g(y) \;dy.$$
What is the derivative of $f$? The Leibniz rule implies that\footnote{Strictly speaking, we should assume for this that $g$ is continuous at $x$ (and also makes ome technical assumptions on $g$ ensuring that the integral exists but such practically irrelevant technicalities are beyond the scope of this short primer). If you go through the next paragraph, you should understand why.}
$$f'(x)=g(x).$$
The way you should think about this is that the integral is the area below the curve. So, draw some (continuous) function $g$ now. Seriously, do it! The integral $\int_0^xg(y)\, dy$ is the area between the axis and the function $g$ from $0$ to some $x$ (take some $x>0$ and shade this area in the graph you just drew). The derivative of $f$ with respect to $x$ gives the answer to the following question: How does the size of the shaded area change if you make $x$ a bit bigger. From the graph, it should be clear that the area gets approximately $g(x)*dx$ bigger if you increase $x$ by $dx$. This means that $f'(x)=g(x)$ (one marginal unit increase in $x$ increases $f$ by $g(x)$).

With a similar intuition the derivative of the function 
\begin{equation*}
  h(x)=\int_x^1g(y)\; dy
\end{equation*}
is 
\begin{equation*}
  h'(x)=-g(x).
\end{equation*}
The complete Leibniz rule (that we won't need) uses a function
\begin{equation*}
  f(x)=\int_{a(x)}^{b(x)}g(x,y)\;dy
\end{equation*}
and says (assuming that all the functions are nicely behaved, i.e. $a$, $b$ and $g$ are differentiable at $x$) that 
\begin{equation*}
  f'(x)=-a'(x) g(x,a(x))+b'(x)g(x,b(x))+\int_{a(x)}^{b(x)}\frac{\partial g}{\partial x}(x,y)\;dy.
\end{equation*}

The result we started with in this section, i.e. $f'(x)=g(x)$, is normally called ``the first fundamental theorem of calculus''. The ``fundamental'' thing about it is that it links the two operations integration and differentiation. The second fundamental theorem of calculus states the following: If $F$ is an antiderivative of $f$ (and $f$ satisfies some technical condition ensuring that the integral $\int_a^b f(x)\,dx$ exists), then $F(b)-F(a)=\int_a^b f(x)\,dx$. Taking these two results together, one can view integration and differentiation as inverse operations. 


\end{document}
