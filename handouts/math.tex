\documentclass[a4paper,12pt]{article}

\usepackage{amsmath,amsthm,amsfonts,dsfont,ae,aecompl}
\usepackage[margin=2.5cm]{geometry}
\usepackage{natbib}
\usepackage{tikz,sgame}

\newtheorem{theorem}{Theorem}

\title{Math for Microeconomics}
\author{Christoph Schottm\"uller}

\begin{document}
\maketitle

\section{Uniform distribution}
\label{sec:uniform-distribution}

The uniform distribution is in some sense the simplest distribution a continuous random variable can have. Recall that a cumulative distribution function (cdf) $F$ of a random variable $X$ gives the answer to the question ``What is the probability that  $X$ is weakly less than some value $x$''. Put differently, $F(x)=0.3$ states that the probability that $X$ is weakly less than $x$ equals $0.3$. Given this interpretation, we immediately know a few properties that a cumulative distribution function has to satisfy:
\begin{enumerate}
\item $F(x)\in[0,1]$: as $F(x)$ is a probability, it has to be between 0 and 1.
\item $F$ is increasing: if the probability that $X\leq x$ is $0.3$, then the probability that $X\leq y$ for $y>x$ has to be at least $0.3$. (Just think about the statement ``The probability that it rains before 3pm is 50\%.'' Logically, this implies that the probability that it rains before 5pm has to be at least 50\%.)
\item If $X$ takes only values in the interval $[a,b]$ (we then call the smallest such interval the \emph{support} of $X$), then $F(x)=0$ for all $x<a$ and $F(x)=1$ for all $x>b$. (If we know for sure that it is raining at some point between 1pm and 3pm, then the probability that it starts raining before 5pm is 1.) 
  \item Let $x>y$. Then $F(x)-F(y)$ is the probability that $X$ is in the interval $(y,x]$. (The probability that it rains before 5pm minus the probability that it rains before 3pm equals the probability that it rains between 3pm and 5pm.)
\end{enumerate}

If the cdf $F$ of the random variable $X$ is continuous and differentiable (almost everywhere), then the derivative of $F$ is called the \emph{density} function of $X$ and typically we denote this density function by $f$. If the support of $X$ is $[a,b]$ and $X$ has the density $f$, then we can write $F(x)=\int_{a}^xf(\tilde x)\,d\tilde{x}$.

The uniform distribution is a particularly simple distribution because its cumulative distribution function is (i) continuous and (ii) linear on its support. In fact, it has a density and the density is constant on the support as the derivative of a linear function is constant. The simplest example of a uniform distribution is the uniform distribution on $[0,1]$ (put differently, we have a random variable $X$ with support $[0,1]$ and and a continuous cdf that is linear on the $[0,1]$). For this uniform distribution on $[0,1]$, we have
$$F(x)=
\begin{cases}
  0 & x<0\\
  x & x\in[0,1]\\
  1 &x>0
\end{cases}$$
and therefore
$$f(x)=
\begin{cases}
  1 & x\in[0,1]\\
  0 & else.
\end{cases}$$

It is tempting to think of a uniform distribution as a distribution for which all values in the support are equally likely. Technically speaking, this intuition is problematic because each value has itself zero probability, i.e. the probability that $X$ equals 0.4546 is zero. Therefore, a better intuition is the following: Fix some interval length, for example, 0.1. The probability that $X$ falls into any subinterval of its support with length 0.1 is the same. That is, the probability that $X$ is in $[0.2,0.3]$ equals the probability that $X$ is in the interval $[0.56,0.66]$ (if both of these intervals are in the support of $X$) because both have length 0.1. If the support of a uniformly distributed random variable has length 1, e.g. in case of the uniform distribution on $[0,1]$, then the length of the subinterval is in fact equal to the probability that $X$ falls into it. For example, the probability that $X$ is in $[0.45,0.49]$ equals $0.04$ in this case.

Finally, let us write down the cdf and density of a uniform distribution on an arbitrary interval $[a,b]$. We know that (i) $F$ is linear on $[a,b]$ and can therefore be written as $F(x)=\alpha+\beta x$ for $x\in[a,b]$, (ii) $F(x)=0$ for $x<a$ and, as $F$ is continuous, $F(a)=0$, (iii) $F(x)=1$ for $x>1$ and $F(b)=1$ by the continuity of $F$. The equations $F(a)=0$ and $F(b)=1$ can be written as
\begin{align*}
  \alpha+\beta a&=0\\
  \alpha+\beta b&=1.
\end{align*}
Solving these equations for $\alpha$ and $\beta$, we obtain
\begin{align*}
  \alpha&=-\frac{a}{b-a}\\
  \beta&=\frac{1}{b-a}.        
\end{align*}
Hence, $F(x)=(x-a)/(b-a)$ and $f(x)=1/(b-a)$ for $x\in[a,b]$.

\subsection{Expected value}
\label{sec:expected-value}

Recall that the expected value of a random variable $X$ with density $f$ and support $[a,b]$ can be computed as $\mathbb{E}[X]=\int_a^bxf(x)\,dx$. For the case of the uniform distribution this yields
$$\int_a^bx\frac{1}{b-a}\,dx=\frac{1}{b-a}\int_a^bx\,dx=\frac{1}{b-a} [\frac{1}{2}x^2]_a^b=\frac{1}{2}\frac{b^2-a^2}{b-a}=\frac{1}{2}\frac{(b-a)(b+a)}{b-a}=\frac{b+a}{2}.$$

This should not be too surprising: The expected value of a uniform distribution is the midpoint of its support. 

\subsection{Conditional expectation}
\label{sec:cond-expect}

The expected value of $X$ conditional on some condition $C$ is written $\mathbb{E}[X|C]$. This expression answers the question ``What is the expected value of X if we know that X satisfies the condition C?'' For example, one might be interested in the expected value of wage conditional on having a university degree. I will focus here on very simple conditions, namely that $X$ is in some interval $[c,d]$. That is, I am interested in $\mathbb{E}[X|c\leq X\leq d]$. For example, one may be interested in the expected wage of those above the poverty line. If the random variable $X$ has density $f$ and support $[a,b]$, then 
$$\mathbb{E}[X|c\leq X\leq d]=\frac{\int_c^dxf(x)\,dx}{F(d)-F(c)}.$$
In case of a uniform distribution and $[c,d]\subseteq[a,b]$, this expression becomes
$$\frac{\int_c^dxf(x)\,dx}{F(d)-F(c)}=\frac{(d^2/2-c^2/2)/(b-a)}{(d-c)/(b-a)}=\frac{1}{2}\frac{d^2-c^2}{d-c}=\frac{1}{2}\frac{(d+c)(d-c)}{d-c}=\frac{d+c}{2}.$$

Again, this should not surprise too much: The expected value of a uniformly distributed variable under the condition that its value is in $[c,d]$ is the midpoint of this interval. 

\section{Implicit function theorem}
\label{sec:impl-funct-theor}

Often a best response function is defined as the solution to a first order condition.
Consider, for example, a Cournot competition model with two firms which both have constant marginal costs $c>0$. Inverse demand is $P(q_1+q_2)$ where we assume that the function $P$ is two times continuously differentiable with $P'<0$ (demand is decreasing) and $P''\leq 0$ (weak concavity). Firm $i\in\{1,2\}$ maximizes $q_i*(P(q_1+q_2)-c)$ over $q_i$. The first order condition of firm 1 is then
\begin{equation*}
  P'(q_1+q_2)q_1+P(q_1+q_2)-c=0.
\end{equation*}
Firm 1's best response to a quantity $q_2$ is the $q_1$ that solves this equation, i.e. firm 1's best response is ``implicitly defined'' by this equation. (It would be explicit if we could solve the equation for $q_1$ but with a general function $P$ we cannot do so.) We make assumptions that guarantee that the equation has only one solution, e.g. $P'<0$ and $P''\leq 0$, which ensures that the implicitly defined best response $q_1(q_2)$ is a well defined function. The implicit function theorem allows us to calculate the slope of this best response function.

\begin{theorem}[1-dimensional Implicit Function Theorem]
  Let $(f_0,x_0)$ be a point such that $g(f_0,x_0)=0$ where $g:\Re^2\rightarrow\Re$ is a continuously differentiable function and $\partial g(f_0,x_0)/\partial f\neq 0$. Then there exists a function $f:(x_0-\varepsilon ,x_0+\varepsilon )\rightarrow\Re$ for some $\varepsilon >0$ such that $f(x)$ is the unique solution to $g(f,x)=0$ for $x\in(x_0-\varepsilon ,x_0+\varepsilon )$ and
  \begin{equation*}
    f'(x)=-\frac{\partial g(f(x),x)/\partial x}{\partial g(f(x),x)\partial f}.
  \end{equation*}
\end{theorem}

Intuitively, we define $f$ as the solution to the equation $g(f,x)=0$. To get the slope of $f$ we totally differentiate this equation
\begin{equation*}
  \frac{\partial g}{\partial f}df+\frac{\partial g}{\partial x}dx=0
\end{equation*}
and rearrange to get $df/dx$. The restriction of the domain to $(x_0-\varepsilon, x_0+\varepsilon )$ and the assumption $\partial g(f_0,x_0)/\partial f\neq 0$ ensure that the equation $g(f,x)$ has a unique solution (on this restricted domain) and therefore the solution to the equation $g(f,x)=0$ is  actually  a function (and not a correspondence). If we knew that $g(f,x)=0$ has a unique solution $f(x)$ for all $x$, then we could leave out the domain restriction.

In the Cournot example, $q_1$ has the role of $f$, $q_2$ the role of $x$ and $g(q_1,q_2)= P'(q_1+q_2)q_1+P(q_1+q_2)-c$. Because of the assumptions $P'<0$ and $P''\leq 0$, we know that $\partial g/\partial q_1<0$ and therefore $g(q_1,q_2)=0$ can have only one solution $q_1$ for a given $q_2$.\footnote{I ignore corner solutions here, i.e. solutions where the first order condition does not hold as an equality. These are relevant for very high $q_2$ where $q_1=0$ can be a best response but ignored here as this has little to do with the implicit function theorem itself.} The theorem then tells us that

\begin{equation*}
   q_1'(q_2)=-\frac{\partial g(q_1(q_2),q_2)/\partial q_2}{\partial g(q_1(q_2),q_2)\partial q_1}=-\frac{P''(q_1(q_2)+q_2)q_1+P'(q_1(q_2)+q_2)}{P''(q_1(q_2)+q_2)+2P'(q_1(q_2)+q_2)}.
\end{equation*}

This is a useful expression because, by our assumptions $P'<0$ and $P''\leq0$, it tells us that $q_1'(q_2)<0$, i.e. firm 1's best response to an increase in firm 2's quantity is a decrease in quantity. (We call this property ``strategic substitutability''.)

\section{Integration by parts}
\label{sec:integration-parts}

This is a little trick that allows you to solve some seemingly tricky integration problems. We will use it a lot in the course. 

You might remember how to integrate simple polynomial functions, e.g.\footnote{Reminder on notation: $[f(x)]_a^b\equiv f(b)-f(a)$.}
$$\int_{2}^{3}x^{2}+1\;dx=\left[ \frac{x^{3}}{3}+x\right]_{2}^{3}=\frac{27}{3}+3-\frac{8}{3}-2=8-\frac{2}{3}.$$
However, constructing the ``antiderivative'' is not always this easy. Think for example of the following integral:
$$\int_{4}^{5} x e^{x}\;dx$$
Here the following little rule helps us:
\begin{theorem}
$\int_{a}^{b}u(x)v'(x)\;dx=\left[u(x)v(x)\right]_{a}^{b}-\int_{a}^{b}u'(x)v(x)\;dx$
\end{theorem} 

Here $u$ and $v$ are (almost everywhere) differentiable functions of $x$. How does this help us? Let's take the example above and say $v'(x)=e^x$ and $u(x)=x$. This implies that $v(x)=e^x$ because the exponential function is its own derivative and $u'(x)=1$. Then the formula above gives us
\begin{equation*}
  \int_4^5 x e^x\;dx=\left[x e^x\right]_4^5-\int_4^5 1*e^x\;dx.
\end{equation*}
This last integral is something we can solve: The antiderivative of $e^x$ is $e^x$ and therefore the last expression can be rewritten as
\begin{equation*}
  = 5 e^5-4 e^4-\left[e^x\right]_4^5=4 e^4-5 e^5-e^5+e^4=5 e^4-6 e^5.
\end{equation*}

\textbf{Another example: }
\begin{equation*}
  \int_a^b x^2 log(x)\;dx
\end{equation*}
Here we use $u(x)=log(x)$ and $v'(x)=x^2$. Therefore, we get $u'(x)=1/x$ and $v(x)=x^3/3$. Plugging this into our rule gives
\begin{align*}
  \int_a^b &x^2 log(x)\;dx=\left[log(x)x^3/3\right]_a^b- \int_a^b\frac{1}{x}\frac{x^3}{3}\;dx=log(b)b^3/3-log(a)a^3/3-\int_a^b\frac{x^2}{3}\;dx\\
&=log(b)b^3/3-log(a)a^3/3-\left[\frac{x^3}{9}\right]_a^b=log(b)b^3/3-log(a)a^3/3-\frac{b^3}{9}+\frac{a^3}{9}.
\end{align*}


\subsection{Why does integration by parts work? (optional and mathematically somewhat loose)}
\label{sec:why-doe-sintegration}
Integration by parts is actually a rule that you already know (but in disguise). You might remember the differentiation rule:
\begin{equation*}
  \left(u(x) v(x)\right)'=u'(x)v(x)+u(x)v'(x)
\end{equation*}

Now rearrange this last equation by subtracting $u'(x)v(x)$ on both sides. This gives
\begin{equation*}
  \left(u(x) v(x)\right)'-u'(x)v(x)=u(x)v'(x).
\end{equation*}
Now integrate both sides from $a$ to $b$:
\begin{equation*}
  \int_a^b\left\{(u(x) v(x))'-u'(x)v(x)\right\}\;dx=\int_a^bu(x)v'(x)\;dx
\end{equation*}
Now we can split up the integral on the left hand side and get
\begin{equation*}
  \int_a^b\left(u(x) v(x)\right)' \;dx-\int_a^bu'(x)v(x)\;dx=\int_a^bu(x)v'(x)\;dx.
\end{equation*}
Taking derivatives and integrating are inverse operations (``they cancel each other out''). Therefore, the first term on the left hand side can be rewritten $\int_a^b\left(u(x) v(x)\right)' \;dx=\left[ u(x) v(x)\right]_a^b$. If you plug this in, you get exactly the integration by parts formula!



\section{Leibniz rule}
\label{sec:leibniz-rule}

The Leibniz rule tells you how to take the derivative of a function that is an integral. We will only use one very special case of it. Therefore, I show you this special case and give the complete rule only for sake of completeness below.

What we want to look at are functions like the following
$$f(x)=\int_0^x g(y) \;dy.$$
What is the derivative of $f$? The Leibniz rule says that\footnote{Strictly speaking, we should assume for this that $g$ is continuous at $x$. If you go through the next paragraph, you should understand why.} 
$$f'(x)=g(x).$$
The way you should think about this is that the integral is the area below the curve. So, draw some (continuous) function $g$ now. Seriously, do it! The integral $\int_0^xg(y)\, dy$ is the area between the axis and the function $g$ from $0$ to some $x$ (take some $x>0$ and shade this area in the graph you just drew). The derivative of $f$ with respect to $x$ gives the answer to the following question: How does the size of the shaded area change if you make $x$ a bit bigger. From the graph, it should be clear that the area gets approximately $g(x)*dx$ bigger if you increase $x$ by $dx$. This means that $f'(x)=g(x)$ (one marginal unit increase in $x$ increases $f$ by $g(x)$).

With a similar intuition the derivative of the function 
\begin{equation*}
  h(x)=\int_x^1g(y)\; dy
\end{equation*}
is 
\begin{equation*}
  h'(x)=-g(x).
\end{equation*}
The complete Leibniz rule (that we won't need) uses a function
\begin{equation*}
  f(x)=\int_{a(x)}^{b(x)}g(x,y)\;dy
\end{equation*}
and says (assuming that all the functions are nicely behaved, i.e. $a$, $b$ and $g$ are differentiable at $x$) that 
\begin{equation*}
  f'(x)=-a'(x) g(x,a(x))+b'(x)g(x,b(x))+\int_{a(x)}^{b(x)}\frac{\partial g}{\partial x}(x,y)\;dy.
\end{equation*}

\textbf{Example: Using integration by parts to eliminate a double integral}

Integration by parts can sometimes be used to transform double integrals to simple integrals:
\begin{equation*}
  \int_0^1\left(2x \int_a^x f(y)\;dy\right)\;dx
\end{equation*}
Here $f(y)$ is some measurable function. We choose $u(x)=\int_a^xf(y)\;dy$ and $v'(x)=2x$. This implies that $u'(x)=f(x)$ (this uses the Leibniz rule above!) and $v(x)=x^2$. Integration by parts gives now
\begin{equation*}
  \int_0^1\left(2x \int_a^x f(y)\;dy\right)\;dx= \left[x^2 \int_a^x f(y)\;dy\right]_0^1-\int_0^1f(x) x^2\;dx=\int_a^1f(y)\;dy-\int_0^1f(x)x^2\;dx.
\end{equation*}

If you check the last expression, you see that we simplified the complicated double intergral we started out with into two simpler ``normal'' integrals.


\end{document}
